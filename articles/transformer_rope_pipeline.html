<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Transformer + RoPE: Full Pipeline</title>
<style>
  @import url('https://fonts.googleapis.com/css2?family=IBM+Plex+Mono:wght@300;400;500;600&family=Sora:wght@300;400;500;600;700&display=swap');

  :root {
    --bg: #07090f;
    --card: #0c1018;
    --border: #161d2d;
    --border-active: #2a3a5c;
    --text: #b8c5d6;
    --text-dim: #4e5f78;
    --text-bright: #e8edf4;
    --accent: #4c9aed;
    --accent-glow: rgba(76, 154, 237, 0.25);
    --green: #34d399;
    --orange: #f59e0b;
    --red: #f87171;
    --purple: #a78bfa;
    --pink: #f472b6;
    --cyan: #22d3ee;
  }

  * { margin: 0; padding: 0; box-sizing: border-box; }

  body {
    background: var(--bg);
    color: var(--text);
    font-family: 'Sora', sans-serif;
    line-height: 1.6;
  }

  .container {
    max-width: 1100px;
    margin: 0 auto;
    padding: 40px 20px 80px;
  }

  h1 {
    font-size: 1.8rem;
    font-weight: 700;
    color: var(--text-bright);
    margin-bottom: 4px;
    letter-spacing: -0.5px;
  }

  .subtitle {
    color: var(--text-dim);
    font-size: 0.9rem;
    margin-bottom: 30px;
    font-weight: 300;
  }

  /* Navigation */
  .step-nav {
    display: flex;
    gap: 0;
    margin-bottom: 36px;
    border: 1px solid var(--border);
    border-radius: 12px;
    overflow: hidden;
    background: var(--card);
  }

  .step-nav-item {
    flex: 1;
    padding: 12px 8px;
    text-align: center;
    font-size: 0.72rem;
    font-family: 'IBM Plex Mono', monospace;
    color: var(--text-dim);
    cursor: pointer;
    transition: all 0.3s;
    border-right: 1px solid var(--border);
    position: relative;
    font-weight: 400;
  }

  .step-nav-item:last-child { border-right: none; }

  .step-nav-item:hover { background: rgba(76, 154, 237, 0.05); }

  .step-nav-item.active {
    background: rgba(76, 154, 237, 0.1);
    color: var(--accent);
    font-weight: 500;
  }

  .step-nav-item.active::after {
    content: '';
    position: absolute;
    bottom: 0;
    left: 0;
    right: 0;
    height: 2px;
    background: var(--accent);
  }

  .step-nav-item .step-num {
    display: block;
    font-size: 0.65rem;
    margin-bottom: 2px;
    opacity: 0.6;
  }

  /* Step content */
  .step-content {
    display: none;
    animation: fadeIn 0.4s ease;
  }

  .step-content.active { display: block; }

  @keyframes fadeIn {
    from { opacity: 0; transform: translateY(10px); }
    to { opacity: 1; transform: translateY(0); }
  }

  .step-title {
    font-size: 1.25rem;
    font-weight: 600;
    color: var(--text-bright);
    margin-bottom: 8px;
    display: flex;
    align-items: center;
    gap: 10px;
  }

  .step-title .tag {
    font-size: 0.65rem;
    font-family: 'IBM Plex Mono', monospace;
    padding: 3px 10px;
    border-radius: 20px;
    font-weight: 500;
  }

  .step-desc {
    color: var(--text-dim);
    font-size: 0.88rem;
    margin-bottom: 24px;
    max-width: 800px;
    line-height: 1.8;
  }

  .step-desc strong { color: var(--text); font-weight: 500; }

  /* Visual panels */
  .visual-panel {
    background: var(--card);
    border: 1px solid var(--border);
    border-radius: 14px;
    padding: 28px;
    margin-bottom: 20px;
    position: relative;
    overflow: hidden;
  }

  .visual-panel::before {
    content: '';
    position: absolute;
    top: 0; left: 0; right: 0;
    height: 1px;
    background: linear-gradient(90deg, transparent, var(--accent), transparent);
    opacity: 0.3;
  }

  .panel-label {
    font-size: 0.68rem;
    font-family: 'IBM Plex Mono', monospace;
    color: var(--text-dim);
    text-transform: uppercase;
    letter-spacing: 1.5px;
    margin-bottom: 16px;
  }

  /* Token display */
  .tokens-row {
    display: flex;
    gap: 8px;
    flex-wrap: wrap;
    align-items: center;
  }

  .token {
    padding: 8px 16px;
    border-radius: 8px;
    font-family: 'IBM Plex Mono', monospace;
    font-size: 0.85rem;
    font-weight: 500;
    transition: all 0.3s;
  }

  .token-word {
    background: rgba(76, 154, 237, 0.12);
    border: 1px solid rgba(76, 154, 237, 0.25);
    color: var(--accent);
  }

  .token-id {
    background: rgba(52, 211, 153, 0.1);
    border: 1px solid rgba(52, 211, 153, 0.25);
    color: var(--green);
  }

  /* Arrow between sections */
  .arrow-down {
    text-align: center;
    padding: 12px 0;
    color: var(--text-dim);
    font-size: 1.2rem;
  }

  /* Vector display */
  .vector-container {
    display: flex;
    gap: 16px;
    flex-wrap: wrap;
  }

  .vector-block {
    flex: 1;
    min-width: 200px;
  }

  .vector-label {
    font-family: 'IBM Plex Mono', monospace;
    font-size: 0.75rem;
    color: var(--text-dim);
    margin-bottom: 8px;
    display: flex;
    align-items: center;
    gap: 8px;
  }

  .vector-label .pos-badge {
    font-size: 0.6rem;
    padding: 2px 6px;
    border-radius: 4px;
    background: rgba(167, 139, 250, 0.15);
    color: var(--purple);
  }

  .vector-cells {
    display: flex;
    gap: 3px;
    flex-wrap: wrap;
  }

  .vector-cell {
    width: 48px;
    height: 32px;
    border-radius: 5px;
    display: flex;
    align-items: center;
    justify-content: center;
    font-family: 'IBM Plex Mono', monospace;
    font-size: 0.62rem;
    font-weight: 400;
    transition: all 0.5s;
  }

  .dim-pair-label {
    display: flex;
    gap: 3px;
    margin-bottom: 4px;
  }

  .dim-pair-bracket {
    display: flex;
    flex-direction: column;
    align-items: center;
    width: 99px;
  }

  .dim-pair-bracket .bracket-text {
    font-size: 0.58rem;
    font-family: 'IBM Plex Mono', monospace;
    color: var(--text-dim);
    white-space: nowrap;
  }

  .dim-pair-bracket .bracket-line {
    width: 80%;
    height: 1px;
    margin-top: 2px;
  }

  /* Matrix display */
  .matrix-grid {
    display: inline-grid;
    gap: 2px;
    margin-top: 8px;
  }

  .matrix-cell {
    width: 52px;
    height: 28px;
    border-radius: 4px;
    display: flex;
    align-items: center;
    justify-content: center;
    font-family: 'IBM Plex Mono', monospace;
    font-size: 0.58rem;
  }

  /* Equation display */
  .equation-box {
    background: rgba(76, 154, 237, 0.06);
    border: 1px solid rgba(76, 154, 237, 0.15);
    border-radius: 10px;
    padding: 16px 20px;
    font-family: 'IBM Plex Mono', monospace;
    font-size: 0.82rem;
    color: var(--text);
    margin: 16px 0;
    line-height: 2;
    overflow-x: auto;
  }

  .eq-highlight { color: var(--accent); font-weight: 500; }
  .eq-green { color: var(--green); }
  .eq-orange { color: var(--orange); }
  .eq-purple { color: var(--purple); }
  .eq-pink { color: var(--pink); }
  .eq-cyan { color: var(--cyan); }

  /* Insight callout */
  .insight {
    background: rgba(52, 211, 153, 0.06);
    border-left: 3px solid var(--green);
    padding: 14px 18px;
    border-radius: 0 10px 10px 0;
    margin: 20px 0;
    font-size: 0.84rem;
    line-height: 1.7;
    color: var(--text);
  }

  .insight strong { color: var(--green); font-weight: 500; }

  .warning {
    background: rgba(245, 158, 11, 0.06);
    border-left: 3px solid var(--orange);
    padding: 14px 18px;
    border-radius: 0 10px 10px 0;
    margin: 20px 0;
    font-size: 0.84rem;
    line-height: 1.7;
  }

  .warning strong { color: var(--orange); font-weight: 500; }

  /* Attention viz */
  .attn-grid {
    display: flex;
    flex-direction: column;
    gap: 2px;
    margin-top: 12px;
  }

  .attn-row {
    display: flex;
    align-items: center;
    gap: 2px;
  }

  .attn-label {
    width: 60px;
    font-family: 'IBM Plex Mono', monospace;
    font-size: 0.65rem;
    color: var(--text-dim);
    text-align: right;
    padding-right: 8px;
    flex-shrink: 0;
  }

  .attn-cell {
    width: 60px;
    height: 30px;
    border-radius: 4px;
    display: flex;
    align-items: center;
    justify-content: center;
    font-family: 'IBM Plex Mono', monospace;
    font-size: 0.6rem;
    transition: all 0.3s;
  }

  /* Flow diagram */
  .flow-row {
    display: flex;
    align-items: center;
    gap: 12px;
    flex-wrap: wrap;
    margin: 12px 0;
  }

  .flow-box {
    padding: 10px 18px;
    border-radius: 8px;
    font-family: 'IBM Plex Mono', monospace;
    font-size: 0.78rem;
    white-space: nowrap;
  }

  .flow-arrow {
    color: var(--text-dim);
    font-size: 1.1rem;
  }

  /* QKV split */
  .qkv-row {
    display: flex;
    gap: 20px;
    flex-wrap: wrap;
    margin: 16px 0;
  }

  .qkv-block {
    flex: 1;
    min-width: 180px;
    background: var(--bg);
    border-radius: 10px;
    padding: 16px;
    border: 1px solid var(--border);
  }

  .qkv-block .qkv-title {
    font-family: 'IBM Plex Mono', monospace;
    font-size: 0.8rem;
    font-weight: 600;
    margin-bottom: 10px;
  }

  /* Softmax bar */
  .softmax-bars {
    display: flex;
    gap: 12px;
    align-items: flex-end;
    height: 100px;
    margin: 16px 0;
    padding: 0 20px;
  }

  .softmax-bar-item {
    display: flex;
    flex-direction: column;
    align-items: center;
    gap: 4px;
    flex: 1;
  }

  .softmax-bar {
    width: 100%;
    max-width: 50px;
    border-radius: 4px 4px 0 0;
    transition: height 0.5s;
  }

  .softmax-bar-label {
    font-family: 'IBM Plex Mono', monospace;
    font-size: 0.62rem;
    color: var(--text-dim);
  }

  .softmax-bar-value {
    font-family: 'IBM Plex Mono', monospace;
    font-size: 0.62rem;
    color: var(--text);
  }

  /* Navigation buttons */
  .nav-buttons {
    display: flex;
    justify-content: space-between;
    margin-top: 32px;
  }

  .nav-btn {
    padding: 10px 24px;
    border-radius: 8px;
    font-family: 'Sora', sans-serif;
    font-size: 0.85rem;
    cursor: pointer;
    transition: all 0.2s;
    border: 1px solid var(--border);
    background: var(--card);
    color: var(--text);
  }

  .nav-btn:hover {
    border-color: var(--accent);
    color: var(--accent);
  }

  .nav-btn.primary {
    background: rgba(76, 154, 237, 0.15);
    border-color: rgba(76, 154, 237, 0.3);
    color: var(--accent);
  }

  .nav-btn:disabled {
    opacity: 0.3;
    cursor: not-allowed;
  }

  .two-col {
    display: grid;
    grid-template-columns: 1fr 1fr;
    gap: 16px;
  }

  @media (max-width: 700px) {
    .two-col { grid-template-columns: 1fr; }
    .step-nav { overflow-x: auto; -webkit-overflow-scrolling: touch; }
    .step-nav::-webkit-scrollbar { height: 0; display: none; }
    .step-nav-item { font-size: 0.58rem; padding: 10px 4px; min-width: 56px; flex: 0 0 auto; white-space: nowrap; }
    .container { padding: 20px 12px 60px; }
    h1 { font-size: 1.4rem; }
    .subtitle { font-size: 0.8rem; }
    .step-title { font-size: 1.1rem; }
    .step-desc { font-size: 0.82rem; }
    .visual-panel { padding: 18px 14px; }
    .equation-box { font-size: 0.72rem; padding: 12px 14px; overflow-x: auto; }
    .insight, .warning { font-size: 0.8rem; padding: 12px 14px; }
    .vector-cell { width: 38px; height: 28px; font-size: 0.55rem; }
    .flow-row { gap: 8px; }
    .flow-box { padding: 8px 12px; font-size: 0.7rem; }
    .qkv-row { gap: 12px; }
    .qkv-block { min-width: 100%; padding: 12px; }
    .attn-cell { width: 48px; height: 26px; font-size: 0.55rem; }
    .attn-label { width: 44px; font-size: 0.58rem; }
    .nav-btn { padding: 8px 16px; font-size: 0.8rem; }
    .pipe-summary { gap: 0; }
    .pipe-step { min-width: 44px; padding: 8px 4px; font-size: 0.6rem; }
    .pipe-step .pipe-icon { font-size: 1rem; }
    .pipe-step .pipe-name { font-size: 0.52rem; }
    .connector { font-size: 0.6rem; padding: 0; }
    .softmax-bars { padding: 0 8px; gap: 6px; }
  }

  @media (max-width: 400px) {
    .container { padding: 14px 8px 50px; }
    h1 { font-size: 1.2rem; }
    .step-nav-item { min-width: 48px; font-size: 0.5rem; }
    .vector-cell { width: 32px; height: 24px; font-size: 0.5rem; }
    .equation-box { font-size: 0.65rem; }
  }

  .rope-animation {
    display: flex;
    align-items: center;
    gap: 20px;
    margin: 16px 0;
    flex-wrap: wrap;
  }

  .rope-anim-canvas {
    border-radius: 10px;
    background: var(--bg);
    border: 1px solid var(--border);
  }

  .dim-color-0 { color: #f87171; }
  .dim-color-1 { color: #fb923c; }
  .dim-color-2 { color: #34d399; }
  .dim-color-3 { color: #4c9aed; }

  .pipe-summary {
    display: flex;
    gap: 0;
    margin: 20px 0;
    flex-wrap: wrap;
  }

  .pipe-step {
    flex: 1;
    min-width: 100px;
    padding: 12px;
    text-align: center;
    position: relative;
    font-size: 0.75rem;
    font-family: 'IBM Plex Mono', monospace;
  }

  .pipe-step .pipe-icon {
    font-size: 1.4rem;
    margin-bottom: 4px;
  }

  .pipe-step .pipe-name {
    color: var(--text-dim);
    font-size: 0.68rem;
  }

  .pipe-step.highlight-pipe {
    background: rgba(76, 154, 237, 0.08);
    border-radius: 8px;
  }

  .connector {
    color: var(--text-dim);
    display: flex;
    align-items: center;
    padding: 0 2px;
    font-size: 0.8rem;
  }
</style>
</head>
<body>
<div class="container">
  <h1>The Full Transformer + RoPE Pipeline</h1>
  <p class="subtitle">Step-by-step: from raw text to output, with actual numbers</p>

  <!-- Pipeline overview -->
  <div class="pipe-summary" id="pipelineSummary">
    <div class="pipe-step" data-step="0"><div class="pipe-icon">üìù</div><div class="pipe-name">Input</div></div>
    <div class="connector">‚Üí</div>
    <div class="pipe-step" data-step="1"><div class="pipe-icon">üî¢</div><div class="pipe-name">Tokenize</div></div>
    <div class="connector">‚Üí</div>
    <div class="pipe-step" data-step="2"><div class="pipe-icon">üìä</div><div class="pipe-name">Embed</div></div>
    <div class="connector">‚Üí</div>
    <div class="pipe-step" data-step="3"><div class="pipe-icon">üîë</div><div class="pipe-name">Q, K, V</div></div>
    <div class="connector">‚Üí</div>
    <div class="pipe-step" data-step="4"><div class="pipe-icon">üîÑ</div><div class="pipe-name">RoPE</div></div>
    <div class="connector">‚Üí</div>
    <div class="pipe-step" data-step="5"><div class="pipe-icon">üéØ</div><div class="pipe-name">Attention</div></div>
    <div class="connector">‚Üí</div>
    <div class="pipe-step" data-step="6"><div class="pipe-icon">‚ö°</div><div class="pipe-name">FFN</div></div>
    <div class="connector">‚Üí</div>
    <div class="pipe-step" data-step="7"><div class="pipe-icon">üí¨</div><div class="pipe-name">Output</div></div>
  </div>

  <!-- Step navigation -->
  <div class="step-nav" id="stepNav">
    <div class="step-nav-item active" data-step="0"><span class="step-num">STEP 0</span>Input Text</div>
    <div class="step-nav-item" data-step="1"><span class="step-num">STEP 1</span>Tokenize</div>
    <div class="step-nav-item" data-step="2"><span class="step-num">STEP 2</span>Embedding</div>
    <div class="step-nav-item" data-step="3"><span class="step-num">STEP 3</span>Q, K, V</div>
    <div class="step-nav-item" data-step="4"><span class="step-num">STEP 4</span>RoPE ‚ú®</div>
    <div class="step-nav-item" data-step="5"><span class="step-num">STEP 5</span>Attention</div>
    <div class="step-nav-item" data-step="6"><span class="step-num">STEP 6</span>FFN + Output</div>
    <div class="step-nav-item" data-step="7"><span class="step-num">STEP 7</span>Full Picture</div>
  </div>

  <!-- ==================== STEP 0: Input ==================== -->
  <div class="step-content active" id="step-0">
    <div class="step-title">Raw Input Text</div>
    <p class="step-desc">
      Everything starts with a string of text. Let's use a simple example with 4 tokens to keep the numbers tractable. We'll use <strong>d=8</strong> (embedding dimension) ‚Äî real models use d=4096+, but the concept is identical.
    </p>
    <div class="visual-panel">
      <div class="panel-label">Input String</div>
      <div style="font-size: 1.4rem; color: var(--text-bright); font-weight: 600; font-family: 'IBM Plex Mono', monospace; letter-spacing: 1px;">
        "The cat sat"
      </div>
      <div style="margin-top: 16px; font-size: 0.8rem; color: var(--text-dim);">
        3 words ‚Üí will become 3 tokens (simplified, no BPE splitting here)
      </div>
    </div>

    <div class="visual-panel">
      <div class="panel-label">What we need to figure out</div>
      <p style="font-size: 0.85rem; line-height: 1.8;">
        The model needs to predict the next token. To do this, it needs to know:<br>
        <span style="color: var(--accent);">1.</span> What each word <strong>means</strong> (content) ‚Üí solved by embeddings<br>
        <span style="color: var(--accent);">2.</span> Where each word <strong>is</strong> (position) ‚Üí solved by RoPE<br>
        <span style="color: var(--accent);">3.</span> How words <strong>relate</strong> to each other ‚Üí solved by attention
      </p>
    </div>
  </div>

  <!-- ==================== STEP 1: Tokenize ==================== -->
  <div class="step-content" id="step-1">
    <div class="step-title">Tokenization <span class="tag" style="background: rgba(52, 211, 153, 0.15); color: var(--green);">STRING ‚Üí IDs</span></div>
    <p class="step-desc">
      The tokenizer splits text into subwords and maps each to an integer ID from its vocabulary. This is a simple lookup ‚Äî no neural network involved yet.
    </p>

    <div class="visual-panel">
      <div class="panel-label">Tokenization Process</div>
      <div class="flow-row">
        <div class="flow-box" style="background: rgba(76, 154, 237, 0.1); border: 1px solid rgba(76, 154, 237, 0.2); color: var(--accent);">"The cat sat"</div>
        <div class="flow-arrow">‚Üí</div>
        <div class="flow-box" style="background: rgba(167, 139, 250, 0.1); border: 1px solid rgba(167, 139, 250, 0.2); color: var(--purple);">Tokenizer Split</div>
        <div class="flow-arrow">‚Üí</div>
        <div style="display: flex; gap: 6px;">
          <span class="token token-word">"The"</span>
          <span class="token token-word">"cat"</span>
          <span class="token token-word">"sat"</span>
        </div>
      </div>

      <div style="margin-top: 20px;"></div>
      <div class="flow-row">
        <div style="display: flex; gap: 6px;">
          <span class="token token-word">"The"</span>
          <span class="token token-word">"cat"</span>
          <span class="token token-word">"sat"</span>
        </div>
        <div class="flow-arrow">‚Üí</div>
        <div class="flow-box" style="background: rgba(245, 158, 11, 0.1); border: 1px solid rgba(245, 158, 11, 0.2); color: var(--orange);">Vocab Lookup</div>
        <div class="flow-arrow">‚Üí</div>
        <div style="display: flex; gap: 6px;">
          <span class="token token-id">464</span>
          <span class="token token-id">2368</span>
          <span class="token token-id">3290</span>
        </div>
      </div>
    </div>

    <div class="visual-panel">
      <div class="panel-label">Position Assignment</div>
      <p style="font-size: 0.82rem; color: var(--text); margin-bottom: 12px;">
        Each token gets a position index. This is what RoPE will use later:
      </p>
      <div style="display: flex; gap: 20px;">
        <div style="text-align: center;">
          <span class="token token-id">464</span>
          <div style="font-family: 'IBM Plex Mono', monospace; font-size: 0.7rem; color: var(--purple); margin-top: 6px;">m = 0</div>
        </div>
        <div style="text-align: center;">
          <span class="token token-id">2368</span>
          <div style="font-family: 'IBM Plex Mono', monospace; font-size: 0.7rem; color: var(--purple); margin-top: 6px;">m = 1</div>
        </div>
        <div style="text-align: center;">
          <span class="token token-id">3290</span>
          <div style="font-family: 'IBM Plex Mono', monospace; font-size: 0.7rem; color: var(--purple); margin-top: 6px;">m = 2</div>
        </div>
      </div>
    </div>

    <div class="insight">
      <strong>Note:</strong> The position m = 0, 1, 2, ... is just the sequential index. At this point, the model has NO awareness of position ‚Äî the token IDs are the same regardless of order. RoPE will fix this in Step 4.
    </div>
  </div>

  <!-- ==================== STEP 2: Embedding ==================== -->
  <div class="step-content" id="step-2">
    <div class="step-title">Token Embedding <span class="tag" style="background: rgba(245, 158, 11, 0.15); color: var(--orange);">IDs ‚Üí VECTORS</span></div>
    <p class="step-desc">
      Each token ID is used to look up a row in the <strong>embedding matrix</strong> (shape: vocab_size √ó d). This converts discrete IDs into continuous vectors that the network can process. These are <strong>learned parameters</strong>.
    </p>

    <div class="visual-panel">
      <div class="panel-label">Embedding Lookup (d = 8)</div>
      <p style="font-size: 0.78rem; color: var(--text-dim); margin-bottom: 16px;">Each token ID ‚Üí row in embedding table ‚Üí 8-dimensional vector</p>
      
      <div class="vector-container" id="embeddingVectors">
        <!-- filled by JS -->
      </div>
    </div>

    <div class="equation-box">
      <span class="eq-highlight">E</span>[464]  ‚Üí [<span class="eq-orange">0.21, -0.53, 0.87, 0.12, -0.44, 0.66, -0.31, 0.09</span>]<br>
      <span class="eq-highlight">E</span>[2368] ‚Üí [<span class="eq-orange">-0.15, 0.72, 0.34, -0.81, 0.55, -0.28, 0.93, -0.47</span>]<br>
      <span class="eq-highlight">E</span>[3290] ‚Üí [<span class="eq-orange">0.63, -0.19, -0.42, 0.58, 0.11, 0.77, -0.66, 0.35</span>]
    </div>

    <div class="warning">
      <strong>Important:</strong> Notice there is <strong>NO position information</strong> in these vectors! "The" gets the same embedding whether it's at position 0 or position 1000. Unlike the original Transformer, RoPE does NOT add position embeddings here ‚Äî it applies them later as rotations on Q and K.
    </div>
  </div>

  <!-- ==================== STEP 3: QKV ==================== -->
  <div class="step-content" id="step-3">
    <div class="step-title">Linear Projections ‚Üí Q, K, V <span class="tag" style="background: rgba(167, 139, 250, 0.15); color: var(--purple);">LEARNED MATRICES</span></div>
    <p class="step-desc">
      Each embedding vector is multiplied by three different learned weight matrices to produce <strong>Query (Q)</strong>, <strong>Key (K)</strong>, and <strong>Value (V)</strong> vectors. Think of it as: Q = "what am I looking for?", K = "what do I contain?", V = "what info do I carry?"
    </p>

    <div class="visual-panel">
      <div class="panel-label">Projection for each token</div>
      <div class="equation-box">
        For each token embedding <span class="eq-highlight">x</span>:<br>
        <span class="eq-pink">q</span> = <span class="eq-highlight">x</span> √ó <span class="eq-pink">W_Q</span> &nbsp;&nbsp;‚Üê "what am I looking for?"<br>
        <span class="eq-cyan">k</span> = <span class="eq-highlight">x</span> √ó <span class="eq-cyan">W_K</span> &nbsp;&nbsp;‚Üê "what do I contain?"<br>
        <span class="eq-green">v</span> = <span class="eq-highlight">x</span> √ó <span class="eq-green">W_V</span> &nbsp;&nbsp;‚Üê "what info do I carry?"
      </div>

      <div class="qkv-row">
        <div class="qkv-block">
          <div class="qkv-title" style="color: var(--pink);">Q (Query)</div>
          <div id="qVectors"></div>
        </div>
        <div class="qkv-block">
          <div class="qkv-title" style="color: var(--cyan);">K (Key)</div>
          <div id="kVectors"></div>
        </div>
        <div class="qkv-block">
          <div class="qkv-title" style="color: var(--green);">V (Value)</div>
          <div id="vVectors"></div>
        </div>
      </div>
    </div>

    <div class="insight">
      <strong>Key point:</strong> Q and K will be rotated by RoPE in the next step. V is <strong>NOT</strong> rotated ‚Äî it stays position-free. This is intentional: position should only affect <strong>which</strong> tokens attend to which (Q¬∑K), not the <strong>content</strong> being passed (V).
    </div>
  </div>

  <!-- ==================== STEP 4: RoPE ==================== -->
  <div class="step-content" id="step-4">
    <div class="step-title">üîÑ RoPE ‚Äî Rotary Position Embedding <span class="tag" style="background: rgba(76, 154, 237, 0.15); color: var(--accent);">THE KEY STEP</span></div>
    <p class="step-desc">
      Now we inject position information by <strong>rotating</strong> Q and K vectors. Each dimension pair (0,1), (2,3), (4,5), (6,7) gets rotated by angle <strong>m √ó Œ∏·µ¢</strong> where m is the position and Œ∏·µ¢ is the frequency for that pair.
    </p>

    <div class="visual-panel">
      <div class="panel-label">Step 4a: Compute rotation frequencies</div>
      <div class="equation-box">
        base = 10000, d = 8, so d/2 = 4 pairs<br><br>
        <span class="dim-color-0">Œ∏‚ÇÄ = 10000^(-0/8) = <strong>1.0000</strong></span> &nbsp;‚Üê fastest<br>
        <span class="dim-color-1">Œ∏‚ÇÅ = 10000^(-2/8) = <strong>0.1000</strong></span><br>
        <span class="dim-color-2">Œ∏‚ÇÇ = 10000^(-4/8) = <strong>0.0100</strong></span><br>
        <span class="dim-color-3">Œ∏‚ÇÉ = 10000^(-6/8) = <strong>0.0010</strong></span> &nbsp;‚Üê slowest
      </div>
    </div>

    <div class="visual-panel">
      <div class="panel-label">Step 4b: Compute rotation angles for each position</div>
      <div class="equation-box">
        angle(m, i) = m √ó Œ∏·µ¢<br><br>
        <strong style="color: var(--accent);">"The" (m=0):</strong> angles = [0√ó1.0, 0√ó0.1, 0√ó0.01, 0√ó0.001] = [<span class="eq-highlight">0, 0, 0, 0</span>] ‚Üê no rotation!<br>
        <strong style="color: var(--accent);">"cat" (m=1):</strong> angles = [1√ó1.0, 1√ó0.1, 1√ó0.01, 1√ó0.001] = [<span class="eq-highlight">1.0, 0.1, 0.01, 0.001</span>] rad<br>
        <strong style="color: var(--accent);">"sat" (m=2):</strong> angles = [2√ó1.0, 2√ó0.1, 2√ó0.01, 2√ó0.001] = [<span class="eq-highlight">2.0, 0.2, 0.02, 0.002</span>] rad
      </div>
    </div>

    <div class="visual-panel">
      <div class="panel-label">Step 4c: Apply 2D rotation to each pair</div>
      <p style="font-size: 0.8rem; color: var(--text-dim); margin-bottom: 16px;">
        For pair i with angle Œ±: &nbsp; <code style="color: var(--accent);">x'‚ÇÄ = x‚ÇÄ¬∑cos(Œ±) - x‚ÇÅ¬∑sin(Œ±)</code> &nbsp; <code style="color: var(--accent);">x'‚ÇÅ = x‚ÇÄ¬∑sin(Œ±) + x‚ÇÅ¬∑cos(Œ±)</code>
      </p>

      <p style="font-size: 0.82rem; color: var(--text); margin-bottom: 12px;">
        <strong>Example: Rotating "cat" (m=1) Query vector</strong>
      </p>
      <div class="equation-box">
        q_cat = [<span class="dim-color-0">0.33, -0.67</span>, <span class="dim-color-1">0.82, 0.15</span>, <span class="dim-color-2">-0.41, 0.59</span>, <span class="dim-color-3">0.73, -0.28</span>]<br><br>
        <span class="dim-color-0">Pair 0 (Œ±=1.0 rad=57.3¬∞):</span><br>
        &nbsp;&nbsp;q'‚ÇÄ = 0.33¬∑cos(1.0) ‚àí (‚àí0.67)¬∑sin(1.0) = 0.33¬∑0.54 + 0.67¬∑0.84 = <strong>0.74</strong><br>
        &nbsp;&nbsp;q'‚ÇÅ = 0.33¬∑sin(1.0) + (‚àí0.67)¬∑cos(1.0) = 0.33¬∑0.84 ‚àí 0.67¬∑0.54 = <strong>‚àí0.08</strong><br><br>
        <span class="dim-color-1">Pair 1 (Œ±=0.1 rad=5.7¬∞):</span> barely rotated<br>
        <span class="dim-color-2">Pair 2 (Œ±=0.01 rad=0.57¬∞):</span> almost unchanged<br>
        <span class="dim-color-3">Pair 3 (Œ±=0.001 rad=0.057¬∞):</span> essentially unchanged
      </div>

      <div id="ropeBeforeAfter"></div>
    </div>

    <div class="insight">
      <strong>What just happened:</strong> The Q and K vectors now carry position info "baked in" via rotation. Token at position 0 is unrotated. Token at position 1 is slightly rotated. Token at position 100 is heavily rotated. But the <strong>magnitude</strong> (length) of each vector is unchanged ‚Äî rotation preserves information, it just encodes position into the <strong>direction</strong>.
    </div>

    <div class="warning">
      <strong>Remember:</strong> V vectors are NOT rotated. Only Q and K get RoPE applied. This way, position only affects attention routing, not the content being passed through.
    </div>
  </div>

  <!-- ==================== STEP 5: Attention ==================== -->
  <div class="step-content" id="step-5">
    <div class="step-title">Attention Scores <span class="tag" style="background: rgba(244, 114, 182, 0.15); color: var(--pink);">Q¬∑K ‚Üí SOFTMAX ‚Üí √óV</span></div>
    <p class="step-desc">
      Now we compute attention: each token's <strong>rotated Q</strong> is dot-producted with every other token's <strong>rotated K</strong>. Because both are rotated by their position, the dot product naturally encodes <strong>relative distance</strong>.
    </p>

    <div class="visual-panel">
      <div class="panel-label">Step 5a: Compute attention scores (Q_rotated √ó K_rotated^T)</div>
      <div class="equation-box">
        score(m, n) = <span class="eq-pink">q_rotated[m]</span> ¬∑ <span class="eq-cyan">k_rotated[n]</span> / ‚àöd<br><br>
        Because of RoPE: score depends on <span class="eq-highlight">(m ‚àí n)</span> not on m and n individually!
      </div>

      <div id="attnScores"></div>

      <div style="margin-top: 16px; font-size: 0.8rem; color: var(--text-dim);">
        Higher score = more attention. Notice diagonal (self-attention) is strongest.
      </div>
    </div>

    <div class="visual-panel">
      <div class="panel-label">Step 5b: Softmax ‚Üí attention weights</div>
      <p style="font-size: 0.8rem; color: var(--text-dim); margin-bottom: 12px;">
        Softmax converts raw scores into probabilities (each row sums to 1):
      </p>
      <div id="attnWeights"></div>
    </div>

    <div class="visual-panel">
      <div class="panel-label">Step 5c: Weighted sum of Values</div>
      <div class="equation-box">
        output[m] = Œ£ attention_weight[m, n] √ó <span class="eq-green">v[n]</span><br><br>
        For "sat" (m=2):<br>
        output = 0.15 √ó <span class="eq-green">v["The"]</span> + 0.30 √ó <span class="eq-green">v["cat"]</span> + 0.55 √ó <span class="eq-green">v["sat"]</span>
      </div>

      <div class="insight">
        <strong>The relative position magic:</strong> "sat" attends most to itself (nearby), less to "cat" (1 step away), and least to "The" (2 steps away). RoPE created this distance-based decay <strong>without any learned position parameters</strong> ‚Äî purely from the geometry of rotation.
      </div>
    </div>
  </div>

  <!-- ==================== STEP 6: FFN + Output ==================== -->
  <div class="step-content" id="step-6">
    <div class="step-title">FFN, LayerNorm & Output <span class="tag" style="background: rgba(52, 211, 153, 0.15); color: var(--green);">FINAL STAGES</span></div>
    <p class="step-desc">
      After attention, each token's output goes through a feedforward network (FFN), residual connections, and layer normalization. This is repeated for every transformer layer. The final layer's output is projected to vocabulary size to predict the next token.
    </p>

    <div class="visual-panel">
      <div class="panel-label">Post-Attention Processing (per token)</div>
      <div class="flow-row">
        <div class="flow-box" style="background: rgba(244, 114, 182, 0.1); border: 1px solid rgba(244, 114, 182, 0.2); color: var(--pink);">Attn Output</div>
        <div class="flow-arrow">‚Üí</div>
        <div class="flow-box" style="background: rgba(167, 139, 250, 0.1); border: 1px solid rgba(167, 139, 250, 0.2); color: var(--purple);">+ Residual</div>
        <div class="flow-arrow">‚Üí</div>
        <div class="flow-box" style="background: rgba(245, 158, 11, 0.1); border: 1px solid rgba(245, 158, 11, 0.2); color: var(--orange);">LayerNorm</div>
        <div class="flow-arrow">‚Üí</div>
        <div class="flow-box" style="background: rgba(52, 211, 153, 0.1); border: 1px solid rgba(52, 211, 153, 0.2); color: var(--green);">FFN</div>
        <div class="flow-arrow">‚Üí</div>
        <div class="flow-box" style="background: rgba(167, 139, 250, 0.1); border: 1px solid rgba(167, 139, 250, 0.2); color: var(--purple);">+ Residual</div>
        <div class="flow-arrow">‚Üí</div>
        <div class="flow-box" style="background: rgba(245, 158, 11, 0.1); border: 1px solid rgba(245, 158, 11, 0.2); color: var(--orange);">LayerNorm</div>
      </div>

      <div class="equation-box" style="margin-top: 16px;">
        <span class="eq-green">FFN(x)</span> = <span class="eq-orange">GELU</span>(x ¬∑ W‚ÇÅ + b‚ÇÅ) ¬∑ W‚ÇÇ + b‚ÇÇ<br><br>
        The FFN expands dim (8 ‚Üí 32) then contracts (32 ‚Üí 8).<br>
        This is where the model does "thinking" ‚Äî transforming representations.
      </div>
    </div>

    <div class="visual-panel">
      <div class="panel-label">Repeat √ó N Layers, then final projection</div>
      <div class="equation-box">
        After all N layers of [<span class="eq-pink">Attention</span> ‚Üí <span class="eq-green">FFN</span>]:<br><br>
        <span class="eq-highlight">logits</span> = final_hidden_state √ó <span class="eq-purple">W_vocab</span> &nbsp;&nbsp;‚Üê shape: (seq_len, vocab_size)<br>
        <span class="eq-highlight">probs</span> = softmax(logits) &nbsp;&nbsp;‚Üê probability over all ~50,000 tokens<br><br>
        For predicting next token after "The cat sat":<br>
        probs["on"] = <strong>0.12</strong>, probs["down"] = <strong>0.09</strong>, probs["there"] = <strong>0.06</strong>, ...
      </div>

      <div style="margin-top: 16px;">
        <div class="softmax-bars" id="outputProbs">
          <!-- filled by JS -->
        </div>
      </div>
    </div>

    <div class="insight">
      <strong>RoPE's role in the big picture:</strong> RoPE only appears in one place ‚Äî rotating Q and K before the attention dot product. But this single operation gives the <strong>entire model</strong> awareness of token ordering. Without it, "The cat sat" and "sat cat The" would produce identical attention patterns and thus identical outputs.
    </div>
  </div>

  <!-- ==================== STEP 7: Full Picture ==================== -->
  <div class="step-content" id="step-7">
    <div class="step-title">The Complete Picture <span class="tag" style="background: rgba(76, 154, 237, 0.15); color: var(--accent);">SUMMARY</span></div>
    <p class="step-desc">
      Let's see the entire pipeline at once and understand where RoPE fits.
    </p>

    <div class="visual-panel">
      <div class="panel-label">End-to-End Pipeline</div>
      <div style="font-family: 'IBM Plex Mono', monospace; font-size: 0.78rem; line-height: 2.6; color: var(--text);">
        <span style="color: var(--text-dim);">‚ë† </span><span style="color: var(--text-bright);">"The cat sat"</span><br>
        <span style="color: var(--text-dim);">&nbsp;&nbsp;&nbsp;‚Üì tokenize</span><br>
        <span style="color: var(--text-dim);">‚ë° </span><span style="color: var(--green);">[464, 2368, 3290]</span> + positions <span style="color: var(--purple);">[0, 1, 2]</span><br>
        <span style="color: var(--text-dim);">&nbsp;&nbsp;&nbsp;‚Üì embedding lookup</span><br>
        <span style="color: var(--text-dim);">‚ë¢ </span><span style="color: var(--orange);">3 vectors of dim 8</span> (content only, no position)<br>
        <span style="color: var(--text-dim);">&nbsp;&nbsp;&nbsp;‚Üì linear projections (W_Q, W_K, W_V)</span><br>
        <span style="color: var(--text-dim);">‚ë£ </span><span style="color: var(--pink);">Q</span>, <span style="color: var(--cyan);">K</span>, <span style="color: var(--green);">V</span> for each token<br>
        <span style="color: var(--text-dim);">&nbsp;&nbsp;&nbsp;‚Üì <span style="color: var(--accent); font-weight: 600;">‚òÖ RoPE: rotate Q and K by position ‚òÖ</span></span><br>
        <span style="color: var(--text-dim);">‚ë§ </span><span style="color: var(--pink);">Q_rot</span> ¬∑ <span style="color: var(--cyan);">K_rot</span><sup>T</sup> ‚Üí scores (encodes relative position!)<br>
        <span style="color: var(--text-dim);">&nbsp;&nbsp;&nbsp;‚Üì softmax ‚Üí attention weights</span><br>
        <span style="color: var(--text-dim);">‚ë• </span>weights √ó <span style="color: var(--green);">V</span> ‚Üí context-aware representations<br>
        <span style="color: var(--text-dim);">&nbsp;&nbsp;&nbsp;‚Üì FFN + residual + layernorm (√ó N layers)</span><br>
        <span style="color: var(--text-dim);">‚ë¶ </span>Final projection ‚Üí <span style="color: var(--text-bright);">next token probabilities</span><br>
        <span style="color: var(--text-dim);">&nbsp;&nbsp;&nbsp;‚Üì sample</span><br>
        <span style="color: var(--text-dim);">‚ëß </span><span style="color: var(--accent); font-weight: 600;">"on"</span> (or "down", "there", ...)
      </div>
    </div>

    <div class="two-col">
      <div class="visual-panel">
        <div class="panel-label">What RoPE does</div>
        <div style="font-size: 0.84rem; line-height: 1.9;">
          <span style="color: var(--green);">‚úì</span> Encodes absolute position as rotation angle<br>
          <span style="color: var(--green);">‚úì</span> Dot product extracts relative distance<br>
          <span style="color: var(--green);">‚úì</span> No learned position parameters<br>
          <span style="color: var(--green);">‚úì</span> Preserves vector magnitude (info)<br>
          <span style="color: var(--green);">‚úì</span> Multi-frequency = multi-scale<br>
          <span style="color: var(--green);">‚úì</span> Compatible with KV caching
        </div>
      </div>

      <div class="visual-panel">
        <div class="panel-label">What RoPE does NOT do</div>
        <div style="font-size: 0.84rem; line-height: 1.9;">
          <span style="color: var(--red);">‚úó</span> Does NOT modify embeddings<br>
          <span style="color: var(--red);">‚úó</span> Does NOT rotate Value vectors<br>
          <span style="color: var(--red);">‚úó</span> Does NOT add anything to vectors<br>
          <span style="color: var(--red);">‚úó</span> Does NOT affect the FFN<br>
          <span style="color: var(--red);">‚úó</span> Does NOT require extra memory<br>
          <span style="color: var(--red);">‚úó</span> Does NOT change between layers
        </div>
      </div>
    </div>

    <div class="visual-panel">
      <div class="panel-label">The One Sentence Summary</div>
      <div style="font-size: 1.05rem; color: var(--text-bright); font-weight: 500; text-align: center; padding: 20px; line-height: 1.8;">
        RoPE rotates Query and Key vectors by their position before the attention dot product,<br>
        so that attention scores naturally depend only on <span style="color: var(--accent);">relative distance</span> between tokens.
      </div>
    </div>
  </div>

  <!-- Navigation buttons -->
  <div class="nav-buttons">
    <button class="nav-btn" id="prevBtn" disabled onclick="navigate(-1)">‚Üê Previous</button>
    <button class="nav-btn primary" id="nextBtn" onclick="navigate(1)">Next Step ‚Üí</button>
  </div>
</div>

<script>
// ---- Data ----
const tokens = ["The", "cat", "sat"];
const tokenIds = [464, 2368, 3290];
const d = 8;
const base = 10000;

// Fake but realistic embeddings
const embeddings = [
  [0.21, -0.53, 0.87, 0.12, -0.44, 0.66, -0.31, 0.09],
  [-0.15, 0.72, 0.34, -0.81, 0.55, -0.28, 0.93, -0.47],
  [0.63, -0.19, -0.42, 0.58, 0.11, 0.77, -0.66, 0.35]
];

// Fake Q, K, V after linear projection
const Q = [
  [0.45, -0.32, 0.71, 0.28, -0.55, 0.43, 0.18, -0.62],
  [0.33, -0.67, 0.82, 0.15, -0.41, 0.59, 0.73, -0.28],
  [-0.22, 0.51, -0.38, 0.69, 0.44, -0.16, 0.57, 0.33]
];

const K = [
  [0.52, 0.19, -0.44, 0.63, 0.31, -0.72, 0.15, 0.48],
  [-0.38, 0.55, 0.27, -0.49, 0.66, 0.13, -0.81, 0.34],
  [0.71, -0.26, 0.58, 0.11, -0.33, 0.85, 0.42, -0.57]
];

const V = [
  [0.31, -0.44, 0.67, -0.22, 0.55, 0.18, -0.73, 0.41],
  [0.58, 0.33, -0.19, 0.72, -0.46, 0.61, 0.27, -0.55],
  [-0.42, 0.66, 0.48, -0.31, 0.19, -0.58, 0.84, 0.13]
];

// Compute thetas
function getThetas() {
  const thetas = [];
  for (let i = 0; i < d / 2; i++) {
    thetas.push(Math.pow(base, -2 * i / d));
  }
  return thetas;
}

// Apply RoPE
function applyRoPE(vec, pos) {
  const thetas = getThetas();
  const result = [...vec];
  for (let i = 0; i < d / 2; i++) {
    const angle = pos * thetas[i];
    const cos_a = Math.cos(angle);
    const sin_a = Math.sin(angle);
    const x0 = vec[2 * i];
    const x1 = vec[2 * i + 1];
    result[2 * i] = x0 * cos_a - x1 * sin_a;
    result[2 * i + 1] = x0 * sin_a + x1 * cos_a;
  }
  return result;
}

const PAIR_COLORS = ['#f87171', '#fb923c', '#34d399', '#4c9aed'];

// ---- Step Navigation ----
let currentStep = 0;
const totalSteps = 8;

function setStep(step) {
  currentStep = step;
  document.querySelectorAll('.step-content').forEach((el, i) => {
    el.classList.toggle('active', i === step);
  });
  document.querySelectorAll('.step-nav-item').forEach((el, i) => {
    el.classList.toggle('active', i === step);
  });
  document.querySelectorAll('.pipe-step').forEach((el) => {
    el.classList.toggle('highlight-pipe', +el.dataset.step === step);
  });
  document.getElementById('prevBtn').disabled = step === 0;
  document.getElementById('nextBtn').disabled = step === totalSteps - 1;
  document.getElementById('nextBtn').textContent = step === totalSteps - 1 ? 'Done ‚úì' : 'Next Step ‚Üí';

  // Render dynamic content
  if (step === 2) renderEmbeddings();
  if (step === 3) renderQKV();
  if (step === 4) renderRoPE();
  if (step === 5) renderAttention();
  if (step === 6) renderOutput();
}

function navigate(dir) {
  const next = currentStep + dir;
  if (next >= 0 && next < totalSteps) setStep(next);
}

document.querySelectorAll('.step-nav-item').forEach(el => {
  el.addEventListener('click', () => setStep(+el.dataset.step));
});

document.querySelectorAll('.pipe-step').forEach(el => {
  el.addEventListener('click', () => setStep(+el.dataset.step));
  el.style.cursor = 'pointer';
});

// ---- Renderers ----
function makeVectorCells(vec, color = 'var(--accent)') {
  return vec.map(v => {
    const intensity = Math.abs(v);
    const isPos = v >= 0;
    const bg = isPos
      ? `rgba(52, 211, 153, ${intensity * 0.4})`
      : `rgba(248, 113, 113, ${intensity * 0.4})`;
    return `<div class="vector-cell" style="background: ${bg}; color: var(--text);">${v.toFixed(2)}</div>`;
  }).join('');
}

function renderEmbeddings() {
  const container = document.getElementById('embeddingVectors');
  container.innerHTML = tokens.map((tok, i) => `
    <div class="vector-block">
      <div class="vector-label">
        "${tok}" <span class="pos-badge">pos ${i}</span>
      </div>
      <div class="vector-cells">${makeVectorCells(embeddings[i])}</div>
    </div>
  `).join('');
}

function renderQKV() {
  ['q', 'k', 'v'].forEach((type, ti) => {
    const data = [Q, K, V][ti];
    const container = document.getElementById(type + 'Vectors');
    container.innerHTML = tokens.map((tok, i) => `
      <div style="margin-bottom: 10px;">
        <div class="vector-label">"${tok}" <span class="pos-badge">m=${i}</span></div>
        <div class="vector-cells">${makeVectorCells(data[i])}</div>
      </div>
    `).join('');
  });
}

function renderRoPE() {
  const container = document.getElementById('ropeBeforeAfter');
  let html = '<div style="margin-top: 20px;">';
  html += '<p style="font-size: 0.82rem; color: var(--text); margin-bottom: 14px;"><strong>Before vs After RoPE on Query vectors:</strong></p>';

  tokens.forEach((tok, i) => {
    const original = Q[i];
    const rotated = applyRoPE(Q[i], i);

    html += `<div style="margin-bottom: 16px;">`;
    html += `<div class="vector-label">"${tok}" (m=${i})</div>`;
    html += `<div style="display: flex; gap: 8px; align-items: center; flex-wrap: wrap;">`;
    html += `<div><div style="font-size: 0.65rem; color: var(--text-dim); margin-bottom: 4px; font-family: 'IBM Plex Mono', monospace;">BEFORE</div>`;
    html += `<div class="vector-cells">`;

    // Show pairs with color coding
    for (let p = 0; p < d / 2; p++) {
      const bg0 = `rgba(${p === 0 ? '248,113,113' : p === 1 ? '251,146,60' : p === 2 ? '52,211,153' : '76,154,237'}, 0.12)`;
      html += `<div class="vector-cell" style="background: ${bg0}; border: 1px solid ${PAIR_COLORS[p]}33;">${original[2*p].toFixed(2)}</div>`;
      html += `<div class="vector-cell" style="background: ${bg0}; border: 1px solid ${PAIR_COLORS[p]}33;">${original[2*p+1].toFixed(2)}</div>`;
    }
    html += `</div></div>`;

    html += `<div style="color: var(--text-dim); font-size: 1rem; padding: 0 4px;">‚Üí</div>`;

    html += `<div><div style="font-size: 0.65rem; color: var(--accent); margin-bottom: 4px; font-family: 'IBM Plex Mono', monospace;">AFTER RoPE</div>`;
    html += `<div class="vector-cells">`;
    for (let p = 0; p < d / 2; p++) {
      const changed = Math.abs(rotated[2*p] - original[2*p]) > 0.01;
      const bg = changed ? `rgba(76, 154, 237, 0.2)` : `rgba(76, 154, 237, 0.06)`;
      const border = changed ? 'border: 1px solid rgba(76,154,237,0.5);' : 'border: 1px solid rgba(76,154,237,0.15);';
      html += `<div class="vector-cell" style="background: ${bg}; ${border}">${rotated[2*p].toFixed(2)}</div>`;
      html += `<div class="vector-cell" style="background: ${bg}; ${border}">${rotated[2*p+1].toFixed(2)}</div>`;
    }
    html += `</div></div>`;
    html += `</div></div>`;
  });

  html += '</div>';
  container.innerHTML = html;
}

function renderAttention() {
  // Compute rotated Q and K
  const Q_rot = Q.map((q, i) => applyRoPE(q, i));
  const K_rot = K.map((k, i) => applyRoPE(k, i));

  // Compute scores
  const scores = [];
  const sqrtD = Math.sqrt(d);
  for (let i = 0; i < 3; i++) {
    scores[i] = [];
    for (let j = 0; j < 3; j++) {
      let dot = 0;
      for (let k = 0; k < d; k++) dot += Q_rot[i][k] * K_rot[j][k];
      scores[i][j] = dot / sqrtD;
    }
  }

  // Softmax per row
  const weights = scores.map(row => {
    const maxVal = Math.max(...row);
    const exps = row.map(v => Math.exp(v - maxVal));
    const sum = exps.reduce((a, b) => a + b, 0);
    return exps.map(e => e / sum);
  });

  // Render scores
  const scoresContainer = document.getElementById('attnScores');
  let html = '<div class="attn-grid">';
  // Header
  html += '<div class="attn-row"><div class="attn-label"></div>';
  tokens.forEach(t => html += `<div class="attn-cell" style="color: var(--text-dim); font-weight: 500;">${t}</div>`);
  html += '</div>';

  scores.forEach((row, i) => {
    html += `<div class="attn-row"><div class="attn-label">${tokens[i]}</div>`;
    row.forEach((v, j) => {
      const intensity = (v - Math.min(...scores.flat())) / (Math.max(...scores.flat()) - Math.min(...scores.flat()));
      const bg = `rgba(76, 154, 237, ${0.1 + intensity * 0.5})`;
      html += `<div class="attn-cell" style="background: ${bg}; color: var(--text-bright);">${v.toFixed(2)}</div>`;
    });
    html += '</div>';
  });
  html += '</div>';
  scoresContainer.innerHTML = html;

  // Render weights
  const weightsContainer = document.getElementById('attnWeights');
  html = '<div class="attn-grid">';
  html += '<div class="attn-row"><div class="attn-label"></div>';
  tokens.forEach(t => html += `<div class="attn-cell" style="color: var(--text-dim); font-weight: 500;">${t}</div>`);
  html += '</div>';

  weights.forEach((row, i) => {
    html += `<div class="attn-row"><div class="attn-label">${tokens[i]}</div>`;
    row.forEach((v, j) => {
      const bg = `rgba(52, 211, 153, ${v * 0.8})`;
      html += `<div class="attn-cell" style="background: ${bg}; color: var(--text-bright); font-weight: 500;">${v.toFixed(2)}</div>`;
    });
    html += '</div>';
  });
  html += '</div>';
  weightsContainer.innerHTML = html;
}

function renderOutput() {
  const container = document.getElementById('outputProbs');
  const predictions = [
    { token: 'on', prob: 0.12 },
    { token: 'down', prob: 0.09 },
    { token: 'there', prob: 0.06 },
    { token: 'quietly', prob: 0.05 },
    { token: 'still', prob: 0.04 },
    { token: 'up', prob: 0.03 },
    { token: 'in', prob: 0.03 },
    { token: '...', prob: 0.58 }
  ];

  container.innerHTML = predictions.map(p => {
    const h = p.prob * 600;
    const alpha = 0.3 + p.prob * 3;
    return `
      <div class="softmax-bar-item">
        <div class="softmax-bar-value">${(p.prob * 100).toFixed(0)}%</div>
        <div class="softmax-bar" style="height: ${h}px; background: rgba(76, 154, 237, ${Math.min(alpha, 1)});"></div>
        <div class="softmax-bar-label">${p.token}</div>
      </div>
    `;
  }).join('');
}

// Init
setStep(0);
</script>
</body>
</html>
